{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import sqlite3\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the .csv'sas Panda's Data Frames\n",
    "tops = pd.read_csv(\"top_airports.csv\")\n",
    "ICAO = pd.read_csv(\"ICAO_airports.csv\")\n",
    "top_50 = pd.read_csv(\"AirportData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to sqlite3 databases\n",
    "\n",
    "#initialize the databases and connection files\n",
    "conn1 = sqlite3.connect(\"/tmp/top_airports.db\")\n",
    "conn2 = sqlite3.connect(\"/tmp/ICAO_airports.db\")\n",
    "conn3 = sqlite3.connect(\"/tmp/AirportData.db\")\n",
    "\n",
    "cursor1 = conn1.cursor()\n",
    "cursor2 = conn2.cursor()\n",
    "cursor3 = conn3.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from Pandas data frame to SQL data base - Only need to run this cell once... ever\n",
    "tops.to_sql('tops_db', conn1)\n",
    "ICAO.to_sql('ICAO_db', conn2)\n",
    "top_50.to_sql('AirportData_db', conn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Lets check if the database was created\n",
    "# sql_cmd = \"SELECT * FROM AirportData_db\"\n",
    "# cursor3.execute(sql_cmd)\n",
    "\n",
    "# db_info = cursor3.fetchall()\n",
    "# for entry in db_info: \n",
    "#      print(entry[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the web crawler function\n",
    "def weather_history(code):\n",
    "   \n",
    "    #ideally one would use the following lines to autofill dates, however, the site is incomplete \n",
    "    #for recent dates so the dates are set to a range that has been tested (last month)\n",
    "    \n",
    "    now = datetime.datetime.now() \n",
    "    month = str(now.month) \n",
    "    #day = str(now.day-7) \n",
    "\n",
    "#assemble url\n",
    "# use this form if auto selecting the past week works\n",
    "# url = 'https://www.wunderground.com/history/airport/' + code + '/2018/' + month + '/' + 'day' + '/WeeklyHistory.html?&reqdb.zip=&reqdb.magic=&reqdb.wmo='\n",
    "    url = 'https://www.wunderground.com/history/airport/' + code + '/2018/2/1/MonthlyHistory.html?&reqdb.zip=&reqdb.magic=&reqdb.wmo='\n",
    "    \n",
    "    #call and read with BS4\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "\n",
    "    #parse the table\n",
    "    table = soup('table')[3]\n",
    "    table_body = table.find_all('tbody')\n",
    "    table_head = table.find('thead')\n",
    "\n",
    "    header = []\n",
    "\n",
    "    for th in table_head.findAll('th'):\n",
    "        key = th.get_text()\n",
    "        header.append(key)\n",
    "\n",
    "    # The first row has an odd sub-header, so it will be left out and re-genreated manually when creating the pandas df\n",
    "    rows = []\n",
    "    for tb in table_body[1:]:\n",
    "        row = []\n",
    "        for elem in tb.find_all('td'):\n",
    "            try:\n",
    "                row.append(float(elem.get_text().strip()))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        rows.append(row)\n",
    "            \n",
    "    #define the column headers\n",
    "    column_head=[ 'February','High Temp','Avg Temp','Low Temp','Low Dew Point','Avg Dew Point','High Dew Point','High Humidity (%)','Avg Humidity (%)','Low HUmidity (%)',\n",
    "            'High Sea Level Press. (in)','Avg Sea Level Press. (in)','Low Sea Level Press. (in)','High Visibiity (mi)','Avg Visibiity (mi)','Low Visibiity (mi)',\n",
    "            'High Wind (mph)','Avg High Wind (mph)','Low High Wind (mph)','Precip. (in)']\n",
    "    df=pd.DataFrame(rows,columns=column_head)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x111f45810>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate the Second Empty Table\n",
    "\n",
    "conn4 = sqlite3.connect(\"/tmp/weathers2.db\") #will create the file if it doesnt alredy exist\n",
    "\n",
    "cursor4 = conn4.cursor()\n",
    "\n",
    "sql_cmd = \"\"\"CREATE TABLE weathers2 (\n",
    "id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT,\n",
    "    day INT,\n",
    "    min_temp FLOAT,\n",
    "    max_temp FLOAT,\n",
    "    humitidy FLOAT,\n",
    "    precipitation FLOAT)\"\"\"\n",
    "\n",
    "\n",
    "cursor4.execute(sql_cmd) #just creates the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use web crawler function to populate the database - may take some time\n",
    "for ii in range(len(top_50)):\n",
    "    if top_50.iloc[ii,1] == 'HNL': #manually check bc wunderground doesnt recognize these IATA codes normally\n",
    "        code = top_50.iloc[ii,1] \n",
    "    elif top_50.iloc[ii,1] =='SJU':\n",
    "        code = top_50.iloc[ii,1]\n",
    "    else: \n",
    "        code = 'K' + top_50.iloc[ii,1] #K's are added to the IATA code for wunderground to recognize\n",
    "    if code == 'KSMF': #bc Sacremento's wunderground table HTML is different from the rest and its currently 1:30 am, skip it.. \n",
    "        continue\n",
    "        \n",
    "    df = weather_history(code) #rn this only runs 1 week back, not 10 years\n",
    "    df = df.fillna('0') #replaces NaN's with back-filled data (NaN's where data is missing from website)\n",
    "    #inserts the following data which is referenced from the structure of the data frame from the web crawler \n",
    "    for jj in range(len(df)):\n",
    "        airport_data = (top_50.iloc[ii,0],df.iloc[jj,0],df.iloc[jj,3],df.iloc[jj,2],df.iloc[jj,8],df.iloc[jj,19])\n",
    "        sql_cmd = (\"INSERT INTO weathers2 (name, day, min_temp, max_temp, humitidy, precipitation) VALUES\" + str(airport_data))\n",
    "        cursor4.execute(sql_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #check if entry worked - run simple query\n",
    "# sql_cmd = \"SELECT * FROM weathers2 WHERE name = 'Hartsfield_Jackson_Atlanta_International'\"\n",
    "# cursor4.execute(sql_cmd)\n",
    "\n",
    "# db_info = cursor4.fetchall()\n",
    "# for entry in db_info: \n",
    "#     print(entry)\n",
    "    \n",
    "# # connection.commit()\n",
    "# # connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data from sqlib as usable form for correlation calculations\n",
    "\n",
    "mintemp_dict={} #pre-define dictionary to store results from all airports\n",
    "maxtemp_dict={}\n",
    "#humid_dict={}\n",
    "precip_dict={}\n",
    "\n",
    "\n",
    "for ii in range(len(top_50)):\n",
    "    if top_50.iloc[ii,1] == 'SMF': #bc Sacremento's was not included above, skip\n",
    "        continue\n",
    "    code = str(top_50.iloc[ii,0])\n",
    "    mintemp_cmd = (\"SELECT weathers2.min_temp FROM weathers2 WHERE name = '\" + str(code) + \"'\")\n",
    "    maxtemp_cmd = (\"SELECT weathers2.max_temp FROM weathers2 WHERE name = '\" + str(code) + \"'\")\n",
    "    precip_cmd = (\"SELECT weathers2.precipitation FROM weathers2 WHERE name = '\" + str(code) + \"'\")\n",
    "    \n",
    "    #extract min temp data\n",
    "    cursor4.execute(mintemp_cmd)\n",
    "    db_info = cursor4.fetchall()\n",
    "    #convert the returned tuple to a useable array format\n",
    "    db_str = str(db_info)\n",
    "    end_db_str=db_str[1:-1]\n",
    "    newstr = end_db_str.replace(\",),\", \",\").replace('(','').replace(',)','').split(', ')\n",
    "    my_array = [float(i) for i in newstr]\n",
    "    my_array = np.asarray(my_array)\n",
    "    aprt = str(code)\n",
    "    mintemp_dict[aprt] = my_array\n",
    "    \n",
    "    #extract max temp data\n",
    "    cursor4.execute(maxtemp_cmd)\n",
    "    db_info = cursor4.fetchall()\n",
    "    #convert the returned tuple to a useable array format\n",
    "    db_str = str(db_info)\n",
    "    end_db_str=db_str[1:-1]\n",
    "    newstr = end_db_str.replace(\",),\", \",\").replace('(','').replace(',)','').split(', ')\n",
    "    my_array = [float(i) for i in newstr]\n",
    "    my_array = np.asarray(my_array)\n",
    "    aprt = str(code)\n",
    "    maxtemp_dict[aprt] = my_array\n",
    "    \n",
    "    \n",
    "    #extract preip. data\n",
    "    cursor4.execute(precip_cmd)\n",
    "    db_info = cursor4.fetchall()\n",
    "    #convert the returned tuple to a useable array format\n",
    "    db_str = str(db_info)\n",
    "    end_db_str=db_str[1:-1]\n",
    "    newstr = end_db_str.replace(\",),\", \",\").replace('(','').replace(',)','').split(', ')\n",
    "    my_array = [float(i) for i in newstr]\n",
    "    my_array = np.asarray(my_array)\n",
    "    aprt = str(code)\n",
    "    precip_dict[aprt] = my_array\n",
    "    \n",
    "#now we have 3 dictionaries with each airport as its own key and the monthly record for that parameter as the entry in array form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.56693587],\n",
       "       [-0.56693587,  1.        ]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the Correlations... didn't get to it ran out of time :(\n",
    "\n",
    "#example -> would run comparisons all airports against each other at different off sets (1-day, 3-day, 7-day)\n",
    "x = mintemp_dict[top_50.iloc[0,0]] # <- daily min temperature for Atlanta for Feb 2018\n",
    "y = mintemp_dict[top_50.iloc[1,0]] # <- daily min temp for Los Angles for Feb 2018\n",
    "corelations = np.corrcoef(x,y)\n",
    "corelations #<- would calc this value for all airport-date relationships with all params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANER - Only Call if ABS necessary\n",
    "\n",
    "def delete_all_tasks(conn):\n",
    "    \"\"\"\n",
    "    Delete all rows in the tasks table\n",
    "    :param conn: Connection to the SQLite database\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql = 'DELETE FROM weathers' #change 'weathers' to which ever database needs cleaning 'AirportData_db'\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "\n",
    "delete_all_tasks(conn4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
