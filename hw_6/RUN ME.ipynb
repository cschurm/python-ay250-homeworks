{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Machine Learning Homework 6\n",
    "Run each cell of the notebook in order, make sure to change \"directory\" in the following cell to the path to your test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/Charlie/Desktop/Tester/\" #define this as YOUR path to the testing folder \n",
    "#Please end this path with '/' or the file finder will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed Utilities and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install PIL / pillow to run this\n",
    "# $ pip install Pillow\n",
    "\n",
    "#Import all the Modules Needed\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage import color\n",
    "from skimage.util.dtype import dtype_range\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.morphology import disk\n",
    "from skimage.filters.rank import gradient\n",
    "from skimage.filters import roberts, sobel, threshold_otsu\n",
    "from skimage.feature import corner_harris, corner_peaks, blob_log, blob_doh, blob_dog\n",
    "from sklearn import model_selection, metrics, cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, metrics, cross_validation, preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction on the Test Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_search(directory):\n",
    "    \"\"\"\n",
    "    This function takes the input \"directory\" which is a file path i.e. \"/new/directory/path/\" to a directory \n",
    "    that contains files like:\n",
    "          validation1.jpg\n",
    "          validation2.jpg\n",
    "          ...\n",
    "    and returns a list with all file paths to images in \"directory\"\n",
    "    \"\"\"\n",
    "    \n",
    "    directory_list = [] #initialize list of lists to collect file names \n",
    "    subdirectories = [f for f in os.listdir(directory) if not f.startswith('.')] #ignores hidden folders\n",
    "    for i in range(len(subdirectories)):\n",
    "        file = subdirectories[i]\n",
    "        file_name = directory+file\n",
    "        directory_list.append(file_name)\n",
    "    return directory_list\n",
    "\n",
    "#Assemble directory List w/above function\n",
    "list_of_paths = directory_search(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(path):\n",
    "    \"\"\"This function takes as input a file path to an image and returns features about that image\"\"\"\n",
    "    #Parse file name\n",
    "    parts = path.split('/')\n",
    "    file_name = parts[-1]\n",
    "    \n",
    "    #Read image\n",
    "    im = Image.open(path)\n",
    "    image = np.array(im)\n",
    "    #Basic Feature Extraction\n",
    "    \n",
    "    #Image Size\n",
    "    y = image.shape[0]\n",
    "    x = image.shape[1]\n",
    "    aspect_ratio = x/y\n",
    "\n",
    "    #Image Color\n",
    "    if len(image.shape) == 2: #image is grey scale\n",
    "        image_gray = image\n",
    "        red_val = 'nan'\n",
    "        grn_val = 'nan'\n",
    "        blu_val = 'nan'\n",
    "        most_freq_color = 'nan' #will clear later\n",
    "    elif len(image.shape) == 3: #image is RGB color\n",
    "    \n",
    "    #Most Frequent Color (really go with 2nd most freq, many of the images will have white borders and make it the most common, which is not helpful)\n",
    "        pixels = im.getcolors(x * y)\n",
    "        most_frequent_pixel = pixels[1]\n",
    "    #Convert RGB Values to single unique number\n",
    "        most_freq_color = 65536*most_frequent_pixel[1][0] + 256*most_frequent_pixel[1][1] + most_frequent_pixel[1][2]\n",
    "\n",
    "    #Avg Value per Color channel\n",
    "        red_val = np.mean(image[:,:,0])\n",
    "        grn_val = np.mean(image[:,:,1])\n",
    "        blu_val = np.mean(image[:,:,2])\n",
    "    \n",
    "    #Grey Scale value \n",
    "    image_gray = color.rgb2gray(image)\n",
    "    grey_val = np.mean(image_gray)\n",
    "\n",
    "    #Outsu Binary Score\n",
    "    thresh = threshold_otsu(image_gray)\n",
    "    binary = image_gray > thresh\n",
    "    binary_per = np.sum(binary)/(x*y) #since we'll be comparing images of different sizes, its more appropriate to use percent of image converted in the binary and not total binary score (# of black pixels)\n",
    "\n",
    "    #Sharpness and Sharpness Frequencies\n",
    "    selection_element = disk(5) # matrix of n pixels with a disk shape\n",
    "    sharpness = (gradient(image_gray, selection_element))\n",
    "    mean_sharp=np.mean(sharpness)\n",
    "#     sharp = sharpness.flatten()\n",
    "#     shrp = np.fft.fft(sharp)\n",
    "#     freq = np.fft.fftfreq(shrp.size, d=1)\n",
    "#     avg_sharp_freq = np.mean(freq)\n",
    "\n",
    "    #Edge Detection\n",
    "    edge_roberts = roberts(image_gray)\n",
    "    edge_sobel = sobel(image_gray)\n",
    "    roberts_score = np.sum(edge_roberts)/(x*y) #normalize to size of image\n",
    "    sobel_score = np.sum(edge_sobel)/(x*y)\n",
    "\n",
    "#     #Blobs - Difference of Gaussian [x coord, y coord, radius size]\n",
    "    blobs_dog = blob_dog(image_gray, max_sigma=30, threshold=.1)\n",
    "#     # use this method for image coordinates for fitting if corners detection fails later\n",
    "    \n",
    "    #Blobs - Laplacian of Gaussian [x coord, y coord, radius size]\n",
    "    blobs_log = blob_log(image_gray, max_sigma=30, num_sigma=10, threshold=.1) \n",
    "    gaussian_blobs = len(blobs_log)\n",
    "    guassian_means = np.mean(blobs_log[:,2])\n",
    "\n",
    "    #Blobs - Determinant of Hessian [x coord, y coord, radius size]\n",
    "    blobs_doh = blob_doh(image_gray, max_sigma=30, threshold=.01) \n",
    "    hessain_blobs = len(blobs_doh)\n",
    "    hessain_means = np.mean(blobs_doh[:,2])\n",
    "    \n",
    "    #Linear Fitting With Corners Detection (Linear Fit to detect corner locations, help determine shape orientation)\n",
    "    coords = corner_peaks(corner_harris(image_gray), min_distance=5)\n",
    "    if coords.shape[0] == 0: #for some reason, corners fails on certain images, use diff method to find feature coordinates\n",
    "        coords=np.empty([len(blobs_dog),2])\n",
    "        coords[:,0] = blobs_dog[:,0]\n",
    "        coords[:,1] = blobs_dog[:,1]\n",
    "        \n",
    "    lin_slope, lin_intercept, lin_r_value, lin_p_value, lin_std_err = stats.linregress(coords[:,0],coords[:,1])\n",
    "    #lin_rsq_value = lin_r_value**2\n",
    "\n",
    "    return [file_name, x, y, aspect_ratio, most_freq_color, red_val, grn_val, blu_val, grey_val,\n",
    "          binary_per, mean_sharp, roberts_score, sobel_score,\n",
    "          lin_slope, lin_intercept, lin_std_err, gaussian_blobs, guassian_means, hessain_blobs, hessain_means]\n",
    "\n",
    "#lin_rsq_value, lin_p_value, avg_sharp_freq : unused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concurrent.futures for parallelizing the feature extraction\n",
    "\n",
    "#Runs the Feature Extraction Function (above) on the given file path for the test set. And returns a csv with the filenames and feature vals\n",
    "#Depending on number of validation files, this may take a few minutes (took ~25 mins on the full 50 categories on my laptop)\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "e = ProcessPoolExecutor()\n",
    "future = e.submit(feature_extraction,list_of_paths)\n",
    "results = list(e.map(feature_extraction, list_of_paths)) \n",
    "#returns a list of lists of the results from feature extraction\n",
    "#collected into a pandas data frame below\n",
    "e.shutdown\n",
    "\n",
    "#Organize Extracted Features\n",
    "columns=['file name','x', 'y','aspect_ratio', 'most_freq_color', 'red_val', 'grn_val', 'blu_val', 'grey_val','binary_per', \n",
    "         'mean_sharp','roberts_score', 'sobel_score', 'lin_slope', 'lin_intercept', 'lin_std_err',\n",
    "         'gaussian_blobs', 'guassian_means', 'hessain_blobs', 'hessain_means']\n",
    "\n",
    "unknown_vals = pd.DataFrame(results,columns=columns)\n",
    "\n",
    "#Save Data to csv so dont have to wait again\n",
    "unknown_vals.to_csv('unknown_vals', sep='\\t')\n",
    "\n",
    "\n",
    "#Ignore the error warnings..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data From Feature Processing\n",
    "train_data = pd.read_csv('category_vals', delimiter = '\\t',index_col=0) #this csv is from feature processing on the full 50 categories folder\n",
    "test_data = pd.read_csv('unknown_vals', delimiter = '\\t',index_col=0) #this is from YOUR testing set\n",
    "\n",
    "#Shuffle the rows of the training data so that categories mix\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "#Some Pre-processing to convert from pandas form to useable np.arrays in sklearn\n",
    "\n",
    "#convert infinities to NaN\n",
    "train_data = train_data.replace([np.inf, -np.inf], np.nan)\n",
    "test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
    "#remove NaNs from data set (set to 0)\n",
    "train_data = train_data.fillna(value=0)\n",
    "test_data = test_data.fillna(value=0)\n",
    "\n",
    "size = train_data.shape \n",
    "Y_pos = size[1]-1 #will use later when extracting the category from the data frame. Categories in the trainging set are in the last column of the dataframe\n",
    "# data_len = size[0]\n",
    "    \n",
    "#split data into X (features) and Y (categories)\n",
    "#remove the file name and the category from the features\n",
    "X_train = train_data.iloc[:,1:-1] \n",
    "X_test = test_data.iloc[:,1:] #testing data does not come with category\n",
    "\n",
    "#Scale the features AND convert the X_ dataframes into an np.arrays useable in sklearn\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)\n",
    "\n",
    "# Extract Categories of the Training Set\n",
    "Y = train_data.iloc[:,Y_pos]\n",
    "Y = Y.as_matrix()\n",
    "\n",
    "#training set\n",
    "Xtr = X_train_scaled\n",
    "Ytr = Y\n",
    "print(\"training size: \" + str(len(Ytr)))\n",
    "# testing set\n",
    "Xte = X_test_scaled\n",
    "print(\"testing size: \" + str(len(Xte)))\n",
    "\n",
    "#Concatenates Features to number of digits useable in sklearn - feature extraction returns values w/too much specificity to be used as dtype float32\n",
    "Xtr = np.around(Xtr, decimals=8)\n",
    "Xte = np.around(Xte, decimals=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier -instantiate classifier object\n",
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "# fit the classification model on training set\n",
    "classifier.fit(Xtr, Ytr)\n",
    "# make predictions for testing set\n",
    "predictions = classifier.predict(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Output File\n",
    "Unknown_Category_Predictions=[]\n",
    "Unknown_Category_Predictions.append(('Filename', 'Predicted Category'))\n",
    "Unknown_Category_Predictions.append(('--'))\n",
    "for i in range(len(predictions)):\n",
    "    parts = list_of_paths[i].split('/')\n",
    "    file_name = parts[-1]\n",
    "    Unknown_Category_Predictions.append((file_name,predictions[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OutPut to Text File\n",
    "with open(\"Unknown_Category_Predictions.txt\", \"w\") as output:\n",
    "    for i in range(len(Unknown_Category_Predictions)):\n",
    "        output.write(str(Unknown_Category_Predictions[i][0])+'       '+str(Unknown_Category_Predictions[i][1])+'\\n')\n",
    "\n",
    "#OutPut to Pandas Data Frame (in csv format)\n",
    "columns = ['Filename','Predicted Category']\n",
    "Unknown_Category_Predictions = Unknown_Category_Predictions[2:]\n",
    "Unknown_Category_Predictions = pd.DataFrame(Unknown_Category_Predictions,columns=columns)\n",
    "Unknown_Category_Predictions.to_csv('Unknown_Category_Predictions', sep='\\t', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
